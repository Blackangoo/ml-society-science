{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from Bernoulli(0.6)\n",
    "import numpy as np\n",
    "xs = np.random.choice(2, p=[0.4, 0.6], size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model description\n",
    "\n",
    "We observe data $D = (x_t)_{t=1}^n$. We assume these come from some Bernoulli distribution with unknown parameter. We can write the model as follows:\n",
    "\n",
    "$\\theta \\sim \\textrm{Beta}(1,1)$\n",
    "\n",
    "$x_t | \\theta \\sim \\textrm{Bernoulli}(\\theta)$\n",
    "\n",
    "The next snippet describes the probabilistic model of $P(\\theta, D)$, in particular, it links the two variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gere we are setting up a model where\n",
    "# theta ~ Beta(1,1)\n",
    "# x_t | theta ~ Bernoulli(theta), with x_t, x_{t-1} independent.\n",
    "jdn = tfd.JointDistributionNamed({\n",
    "    'theta': tfd.Beta(concentration0=1, concentration1=1),\n",
    "    'x_t': lambda theta : tfd.Independent(\n",
    "        tfd.Bernoulli(probs=theta)\n",
    "    )\n",
    "})\n",
    "# can't get reinterpreted_batch_ndims=1 to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.53497887, 0.93697095], dtype=float32)>,\n",
       " 'x_t': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of sampling\n",
    "jdn.sample(2) # take two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can pick just one variable from the sample\n",
    "jdn.sample(1)['x_t'] # take one sample, get only 'x_t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimation\n",
    "\n",
    "First, we generate some data.\n",
    "Then, we fix the values of some variables to be equal to the data values, and sample from the rest. \n",
    "It would have been nice if tfd allowed us to simply do tf.sample(x_t = data), but we must do something more complicated, which has the same effect.\n",
    "\n",
    "\n",
    "Here we define:\n",
    "\n",
    "$\\ln \\mathbb{P}_\\xi(\\theta, D) = \\ln P_\\theta(D) \\xi(\\theta) = \\ln \\prod_t P_\\theta(x_t) \\xi(\\theta) = \\sum_t \\ln P_\\theta(x_t) + \\ln \\xi(\\theta)$,\n",
    "\n",
    "or in alternative notation, where we just use $\\Pr$ loosely,\n",
    "\n",
    "$\\Pr(\\theta, D) = \\Pr(D | \\theta) \\Pr(\\theta) = \\prod_t \\Pr(x_t | \\theta) \\Pr(\\theta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFP makes no distinction between data and parameters.\n",
    "# They are all just random variables.\n",
    "# So we need to fix the value of the observed variables to equal the data values.\n",
    "def target_log_prob(theta):\n",
    "    return tf.reduce_sum(jdn.log_prob(theta=theta, x_t = xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-70.4067>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_log_prob(0.7) # this is the probability of X_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum A Posteriori Estimate\n",
    "\n",
    "$\\arg\\max_\\theta \\xi(\\theta  | D) = \\arg \\max_\\theta P_\\theta(D) \\xi(\\theta) / \\mathbb{P}_\\xi(D) = \\arg \\max_\\theta P_\\theta(D) \\xi(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(0.5)\n",
    "loss = lambda: -target_log_prob(theta)\n",
    "opt = tf.optimizers.SGD(learning_rate=0.001)  #note that this does not take into account constraints\n",
    "loss_hist = []\n",
    "var_hist = []\n",
    "for _ in range(100):\n",
    "    opt.minimize(loss, theta)\n",
    "    loss_hist.append(loss().numpy())\n",
    "    var_hist.append(theta.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(var_hist)\n",
    "np.mean(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling_kernel = tfp.mcmc.RandomWalkMetropolis(target_log_prob)\n",
    "sampling_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn = target_log_prob,\n",
    "    step_size=0.01,\n",
    "    num_leapfrog_steps=2\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap sample_chain in tf.function, telling TF to precompile a reusable\n",
    "# computation graph, which will dramatically improve performance.\n",
    "@tf.function\n",
    "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
    "  return tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=initial_state,\n",
    "    kernel=sampling_kernel,\n",
    "    trace_fn=lambda current_state, kernel_results: kernel_results)\n",
    "dtype=np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = dtype(0.5)\n",
    "samples, kernel_results = run_chain(initial_state, num_results=10000,num_burnin_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
       "array([0.59211165, 0.5792662 , 0.57102245, ..., 0.5923819 , 0.5712529 ,\n",
       "       0.582647  ], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
