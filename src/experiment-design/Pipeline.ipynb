{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will set up a pipeline for\n",
    "1. Collecting data for an experiment\n",
    "2. Processing the data\n",
    "3. Running a machine learrning algorithm.\n",
    "4. Evaluating the algorithm\n",
    "5. Choosing the best algorithm and hyperparameters\\\n",
    "\n",
    "The main questions we need to answer are\n",
    "1. How will data be collected?\n",
    "2. How much data would we need?\n",
    "3. What algorithm would be best? \n",
    "4. How would the amount of data influence algorithm selection?\n",
    "5. How robust is our procedure to assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "Here we want to call a simulator that collects data for us. The simulator can be arbitrary, but we would normally wish to have a specific API for calling it. In the simplest case, the only input parameter is the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_generator\n",
    "\n",
    "# Obtain data from the data generator. \n",
    "# In this case, the data generator gives us a random sample. Other sampling methodologies are possible, of course.\n",
    "generator = data_generator.GaussianClassificationGenerator(2, np.array([0.3, 0.7]))\n",
    "[X, Y] = generator.generate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Here we perform preliminary processing of the data, if necessary. In particular, we may want to split the data in training, validation and test sets. Other standard pre-processing includes scaling the data, dealing with missing data, and removal of problematic data points. However, all of these could theoretically be dealt within the learning algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "Here we run an ML algorithm, using the data for finding appropriate parameters. It is best to have a unified interface for doing this as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
