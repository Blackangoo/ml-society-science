{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/unine.png\" width=\"200\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/fair_1.png\" width=\"200\"/>\n",
    "</figure>\n",
    "\n",
    "# Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (ML) usage is increasing every day. Companies and organisations build ML models with the purpose of minimising human effort or improving performance in various tasks. So today Machine learning arguably affects our lives.\n",
    "\n",
    "Examples:\n",
    "1. Recommendation system\n",
    "2. Face recognition\n",
    "3. Self-driving cars\n",
    "4. Candidate selection e.x college admissions, CV screening\n",
    "5. Loan admissions\n",
    "\n",
    "There are a lot of examples that researchers demonstrate inadvertently discriminating against several population groups.    \n",
    "The most know is [ProPublica's](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) research in COMPAS Dataset.\n",
    "\n",
    "When machine learning models are being used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both performance and fairness. \n",
    "\n",
    "So in the last decade Fairness has become one of the most active research areas in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Sources of Bias.\n",
    "\n",
    "Bias may be introduced into a machine learning project at any step along the way, and it is important to carefully think through each potential source and how it may affect your results.\n",
    "\n",
    "\n",
    "Source of bias:\n",
    "- Data\n",
    "    1. Historical injustice.\n",
    "    2. Sample bias - Collection Bias.\n",
    "    3. Limited features.\n",
    "    4. Unbalanced dataset.\n",
    "    5. Proxy Variables.\n",
    "- Modelling\n",
    "    1. Data preprocessing\n",
    "    2. Model assumptions \n",
    "- Feedback loops.\n",
    "    1. decisions based on biassed models lead to biassed dataset.  \n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/simple_pipeline.png\" width=\"600\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Load Dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_types = OrderedDict([\n",
    "    (\"age\", \"int\"),\n",
    "    (\"workclass\", \"category\"),\n",
    "    (\"final_weight\", \"int\"),  # originally it was called fnlwgt\n",
    "    (\"education\", \"category\"),\n",
    "    (\"education_num\", \"int\"),\n",
    "    (\"marital_status\", \"category\"),\n",
    "    (\"occupation\", \"category\"),\n",
    "    (\"relationship\", \"category\"),\n",
    "    (\"race\", \"category\"),\n",
    "    (\"sex\", \"category\"),\n",
    "    (\"capital_gain\", \"float\"),  # required because of NaN values\n",
    "    (\"capital_loss\", \"int\"),\n",
    "    (\"hours_per_week\", \"int\"),\n",
    "    (\"native_country\", \"category\"),\n",
    "    (\"income_class\", \"category\"),\n",
    "])\n",
    "target_column = \"income_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        names=data_types,\n",
    "        index_col=None,\n",
    "\n",
    "        comment='|',  # test dataset has comment in it\n",
    "        skipinitialspace=True,  # Skip spaces after delimiter\n",
    "        na_values={\n",
    "            'capital_gain': 99999,\n",
    "            'workclass': '?',\n",
    "            'native_country': '?',\n",
    "            'occupation': '?',\n",
    "        },\n",
    "        dtype=data_types,\n",
    "    )\n",
    "\n",
    "def clean_dataset(data):\n",
    "    # Test dataset has dot at the end, we remove it in order\n",
    "    # to unify names between training and test datasets.\n",
    "    data['income_class'] = data.income_class.str.rstrip('.').astype('category')\n",
    "    \n",
    "    # Remove final weight column since there is no use\n",
    "    # for it during the classification.\n",
    "    data = data.drop('final_weight', axis=1)\n",
    "    \n",
    "    # Duplicates might create biases during the analysis and\n",
    "    # during prediction stage they might give over-optimistic\n",
    "    # (or pessimistic) results.\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # Binary target variable (>50K == 1 and <=50K == 0)\n",
    "    data[target_column] = (data[target_column] == '>50K').astype(int)\n",
    "    \n",
    "    # Categorical dataset\n",
    "    categorical_features = data.select_dtypes('category').columns\n",
    "    data[categorical_features] = data.select_dtypes('category').apply(lambda x: x.cat.codes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and clean train dataset\n",
    "TRAIN_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "train_data = clean_dataset(read_dataset(TRAIN_DATA_FILE))\n",
    "train_data = train_data.dropna()\n",
    "print(\"Train dataset shape:\", train_data.shape)\n",
    "\n",
    "# get and clean test dataset\n",
    "TEST_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_data = clean_dataset(read_dataset(TEST_DATA_FILE))\n",
    "test_data = test_data.dropna()\n",
    "print(\"Test dataset shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(x=train_data[\"income_class\"])\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(x=train_data[\"sex\"])\n",
    "fig.set_xticklabels(['woman','men'])\n",
    "# plt.title(\"gender\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(data=train_data[[\"income_class\",\"sex\"]],\n",
    "                    x=\"income_class\",\n",
    "                    hue=\"sex\")\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.legend(title='sex', labels=['woman', 'men'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite imbalance between a gender, so we expect our model to be unfair.\n",
    "The source of the bias is coming from our societal bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4. Train a Model\n",
    "let's train our model and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns.difference([target_column])\n",
    "\n",
    "# we define sex as our sensitive feature\n",
    "sensitive_feature = [\"sex\"]\n",
    "\n",
    "# all the rest are non sensitive feature\n",
    "non_sensitive_features = list(set(features).difference(set(sensitive_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                max_depth=10)\n",
    "\n",
    "model.fit(X = train_data[sensitive_feature+non_sensitive_features],\n",
    "          y = train_data[target_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure performance scores\n",
    "\n",
    "Bellow are some of the most know mesurements scores\n",
    "\n",
    "\n",
    ">1. Accuracy: $\\frac{TP+TN}{TN + FP + FN + TP}$  \n",
    "2. Recall: $\\frac{TP}{ TP + FN }$  \n",
    "3. Precision: $\\frac{TP}{ TP + FP }$  \n",
    "4. F1-score: $2* \\frac{ Recall * Precision}{Precision + Recall}$  \n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/conf_matrix.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true = test_data[target_column],\n",
    "                               y_pred = model.predict(test_data[sensitive_feature+non_sensitive_features]))\n",
    "conf_matrix = pd.DataFrame(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Map\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = conf_matrix.values.ravel()\n",
    "\n",
    "acc_score = \n",
    "precision_score = \n",
    "recall_score = \n",
    "f1_score = \n",
    "\n",
    "print(\"accuracy score:\", acc_score)\n",
    "print(\"precision score:\", precision_score)\n",
    "print(\"recall score:\", recall_score)\n",
    "print(\"f1 score:\", recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_true = test_data[target_column],\n",
    "                           y_pred = model.predict(test_data[sensitive_feature+non_sensitive_features]))\n",
    "print(\"Accuracy_score:\", acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5. Analysis of model fairness\n",
    "\n",
    "\n",
    "Unfortunately there is not a single definition of fairness. There are many, often competing, ways to measure whether a given model is statistically “fair”, but it’s important to remember to start from the social and policy goals for equity and fairness and map those to the statistical properties we want in our models to help achieve those goals. Most of the definitions involve splitting the population into groups and compare metrics on those groups.\n",
    "\n",
    "Different definition preserve different fairness aspects. So the desired fairness definition is depending on the application. In general, this requires consideration of the project’s goals, and a detailed discussion between the data scientists, decision makers, and those who will be affected by the application of the model.\n",
    "\n",
    "Bellow we will define the most known fairness definitions.\n",
    "\n",
    "1. **Demographic Parity** - equal positive outcome rates\n",
    "2. **Equalized Opportunities** - equal true positive  rates\n",
    "3. **Equalized Odds** -  equal true positive and false positive rate\n",
    "4. **Predictive Parity** - equal Positive precision  \n",
    ".  \n",
    ".  \n",
    ".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "predictions = pd.Series(predictions,index = test_data.index)\n",
    "sensitive_atribute = test_data[sensitive_feature[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_mask = sensitive_atribute == 0\n",
    "conf_matrix_0 = confusion_matrix(y_true = test_data.loc[woman_mask,target_column],\n",
    "                                 y_pred = predictions.loc[woman_mask])\n",
    "conf_matrix_0 = pd.DataFrame(conf_matrix_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_mask = sensitive_atribute == 1\n",
    "conf_matrix_1 = confusion_matrix(y_true = test_data.loc[man_mask,target_column],\n",
    "                                 y_pred = predictions.loc[man_mask])\n",
    "conf_matrix_1 = pd.DataFrame(conf_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (8,4))\n",
    "sns.heatmap(conf_matrix_0, annot=True, fmt='g', cbar=False, cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Woman Confusion matrix\")\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "\n",
    "sns.heatmap(conf_matrix_1, annot=True, fmt='g', cbar=False, cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title(\"Men Confusion matrix\")\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Prediction')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1.  Demographic Parity\n",
    "\n",
    "**Demographic parity** , also referred to as **statistical parity** , **acceptance rate parity**  and **benchmarking**\n",
    "\n",
    "Demographic Parity states that the positive outcome rates between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Demographic Parity :** A classifier $C$ is said that satisfy **demographic parity** if $$ P_c(\\hat{Y}= 1 | S = s ) =  P_c(\\hat{Y} = 1), \\quad  \\forall s\\in S $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we can also equivalent write:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0 ) =  P_c(\\hat{Y}= 1 | S = 1 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Demographic parity?**\n",
    "\n",
    "we can measure the Demographic parity using the Demographic parity difference metric.\n",
    "\n",
    "> **Demographic parity difference :** we can measure the demographic parity using the next formula:\n",
    "  $$  \\text{Metric} =  max_s[P_c(\\hat{Y}= 1 | S = s )] -  min_s[P_c(\\hat{Y}= 1 | S = s )] $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of demographic parity difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the propotion of accepted rate.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the proportion of accepted rate.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    proportion_of_accepted_rate = []\n",
    "    for group in unique_groups:\n",
    "        pred_group = predictions[sensitive_attribute==group]\n",
    "        accepted_rate = #Calculate the accepted rate\n",
    "        proportion_of_accepted_rate += [accepted_rate]\n",
    "\n",
    "    # 2. Find the maximum and the minimum accepted_rate.\n",
    "    maximum_accepted_rate = max(proportion_of_accepted_rate)\n",
    "    minimum_accepted_rate = min(proportion_of_accepted_rate)\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference = maximum_accepted_rate - minimum_accepted_rate\n",
    "    \n",
    "    return difference, proportion_of_accepted_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_metric, rates = demographic_parity_difference(predictions, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Demographic parity difference is: {round(demographic_metric,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The rate of different groups are: {rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use demographic parity?** \n",
    "1. We are aware of **historical biases** that may have affected the quality of our data. E.x not presents of specific minority in a specific work.\n",
    "2. We want to **change the state of our current world to improve it**. For example, we want to have equal admitions of different group in a specific project.\n",
    "\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Laziness**: We can satisfy demographic parity in we accept random people in group S=0 but qualified people in group  b unless we have the same proportion of positive outcome. We avoid lasyness because classfication is usually perfomred by optimizing an perfomance metric.\n",
    "2. **Not optimality compatible**: A classifier that satisfy demographic parity is suboptimal, if the dataset demographic parity is not hold.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2. Equalized Opotunities\n",
    "\n",
    "**Equalized Opportunities** , also referred to as **true positive parity**, **sensitivity**.\n",
    "\n",
    "Equalized Opportunities states that each group has equal true positive rates.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Opotunities :** A classifire $C$ is said that satisfy **equalized opotunities** if $$ P_c(\\hat{Y} = 1 | S = s, Y = 1 ) =  P_c(\\hat{Y} = 1 | Y = 1), \\quad  \\forall s\\in S $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we can also equivalent write:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Equalized Opportunities?**\n",
    "\n",
    "we can measure the equalized Opportunities using the equalized opportunities difference metric.\n",
    "\n",
    "> **equalized opportunities difference :** we can measure the equalized opportunities using the next formula:\n",
    "  $$  \\text{Metric} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "> **True Positive Rate** : To calculate the $P_c(\\hat{Y}= 1 | Y = 1 )$ you can do the following:\n",
    "  $$  P_c(\\hat{Y}= 1 | Y = 1 ) = \\frac{P_c(\\hat{Y}= 1, Y = 1 )}{P_c( Y = 1 )} = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_opportunities_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized opportunities difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calculate the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the TPR.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    true_positive_rates = []\n",
    "    for group in unique_groups:\n",
    "        # calculate the true positve rate and store it in the true_positive_rates list\n",
    "\n",
    "    # 2. Find the maximum and the minimum accepted_rate.\n",
    "    maximum_tpr = max(true_positive_rates)\n",
    "    minimum_tpr = min(true_positive_rates)\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference = maximum_tpr - minimum_tpr\n",
    "    \n",
    "    return difference, true_positive_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]\n",
    "actual = test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_opportunities_metric, _ = equalized_opportunities_difference(predictions, actual, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized opportunities difference is: {round(equalized_opportunities_metric,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use equalized opportunities?**\n",
    "1. When is a strong emphasis on predicting the positive outcome correctly. e.x college admissions\n",
    "2. When False Positives are not costly for the stakeholder. e.x spam detection\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Not optimality compatible**: A classifier that satisfy demographic parity is suboptimal, if the dataset demographic parity is not hold.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Equalized Odds\n",
    "\n",
    "**Equalized Odds** , also referred to as\n",
    "\n",
    "Equalized Odds states that the true positive rates (TPR) and false positive rates (FPR) between sensitive group must be the same.\n",
    "\n",
    "- TPR = TP / total_positives  = TP / (TP+FN)\n",
    "- FPR = FP / total_negatives = FP / (FP+TN)\n",
    "---------------\n",
    "> **Definition Equalized Odds :** A classifier $C$ is said that satisfy **equalized odds** if $$ P_c(\\hat{Y} = 1 | S = s, Y = y ) =  P_c(\\hat{Y} = 1 | Y = y), \\quad  \\forall s\\in S, \\forall y\\in Y $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we want to satisfy both:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$ \n",
    "and \n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 0 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 0 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Equalized Opportunities?**\n",
    "\n",
    "we can measure the equalized Opportunities using the equalized opportunities difference metric.\n",
    "\n",
    "> **equalized opportunities difference :** we can measure the equalized opportunities using the next formula:\n",
    "  $$   \\text{Metric} = \\text{true_positive_rate_difference} + \\text{false_positive_rate_difference}  $$\n",
    "where\n",
    "  $$ \\text{true_positive_rate_difference} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] $$ \n",
    "  $$ \\text{false_positive_rate_difference} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 0 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 0 )] $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized odds difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the TPR.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    true_positive_rates = []\n",
    "    false_positive_rates = []\n",
    "    for group in unique_groups:\n",
    "        # calculate true positive and false positive rate and store in the list\n",
    "\n",
    "    # 2. Find the maximum and the minimum rates.\n",
    "    maximum_tpr = # find the max true positive rates\n",
    "    minimum_tpr = # find the min true positive rates\n",
    "    \n",
    "    maximum_fpr = # find the max false positive rates\n",
    "    minimum_fpr = # find the min false positive rates\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference_tpr = # calculate difference of true positive rates\n",
    "    difference_fpr = # calculate difference of false positive rates\n",
    "    difference = # sum the difference\n",
    "    \n",
    "    return difference, (true_positive_rates,false_positive_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]\n",
    "actual = test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_metrics, _ = equalized_odds_difference(predictions, actual, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized odds difference is: {round(equalized_odds_metrics,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([acc_score,\n",
    "                        precision_score,\n",
    "                        recall_score,\n",
    "                        f1_score,\n",
    "                        demographic_metric,\n",
    "                        equalized_opportunities_metric,\n",
    "                        equalized_odds_metrics], \n",
    "                       index = [\"accuracy\",\n",
    "                                \"precision\",\n",
    "                                \"recall\",\n",
    "                                \"f1_score\",\n",
    "                                \"demographic_metric\",\n",
    "                                \"equalized_opportunities\",\n",
    "                                \"equalized_odds\"],\n",
    "                       columns = [\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "**When to use equalized odds?** \n",
    "1. When is a strong emphasis on predicting the positive outcome correctly. e.x college admissions\n",
    "2. When we strongly care about minimising costly False Positives\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Not optimality compatible**: A classifier that satisfy equalized odds is suboptimal, if the dataset demographic parity is not hold. Its more constrained than the equal opportunities, so the effect on perforce will be stronger\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your project you have explained why you choose the specific fairness criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Try to fix unfairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(y_pred, y_true, sensitive_feature):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true = y_true,\n",
    "                                   y_pred = y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix)\n",
    "\n",
    "    TN, FP, FN, TP = conf_matrix.values.ravel()\n",
    "\n",
    "    acc_score = ( TN + TP ) / (TN + FP + FN + TP)\n",
    "    precision_score = TP  / (TP + FP)\n",
    "    recall_score = TP  / (TP + FN)\n",
    "    f1_score = 2*(precision_score*recall_score)/(precision_score+recall_score)\n",
    "    \n",
    "    demographic_metric, _ = demographic_parity_difference(y_pred,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    equalized_opportunities_metric, _ = equalized_opportunities_difference(y_pred,\n",
    "                                                                           y_true,\n",
    "                                                                          sensitive_feature)\n",
    "    equalized_odds_metrics, _ = equalized_odds_difference(y_pred,\n",
    "                                                          y_true,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    return {\n",
    "            \"accuracy\": acc_score,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"demographic_metric\": demographic_metric,\n",
    "            \"equalized_opportunities\" : equalized_opportunities_metric,\n",
    "            \"equalized_odds\": equalized_odds_metrics\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1. Unawareness\n",
    "A very intuitive but wrong solution is to assume that ignoring the sensitive feature we can build a fair classifier.\n",
    "This approach is wrong because other feature may be correlated with our sensitive features.\n",
    "\n",
    "We will build a **Unawareness** classifier and the compare the results with our previous classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "unaware_model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                        max_depth=10)\n",
    "\n",
    "unaware_model.fit(X = train_data[non_sensitive_features],\n",
    "                  y = train_data[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_model.predict(test_data[non_sensitive_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_metrics = compute_metric(y_pred = unaware_model.predict(test_data[non_sensitive_features]),\n",
    "                                 y_true = test_data[target_column],\n",
    "                                 sensitive_feature = test_data[\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_results = pd.DataFrame(unaware_metrics,\n",
    "                               index = [\"unaware_model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([results, unaware_results],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Our conclusion is that we are not able to vanish unfairness by excluding sensitive features.  \n",
    "The reason in that some other feature leak information about our sensitive attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 Post processing\n",
    "From the above results we expect that our sensitive feature are correlate with non-sensitive feature in our analysis.\n",
    "We will try to find and exclude also those feature in order to improve our fairness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {}\n",
    "for f in non_sensitive_features:\n",
    "    corr[f] =np.corrcoef(test_data[sensitive_feature[0]], test_data[f])[0,1]\n",
    "corr = np.abs(pd.DataFrame(data=corr, index=[\"corr\"]).T).sort_values(by=\"corr\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "corr.plot(kind=\"bar\")\n",
    "plt.title(\"Correlation of sensitive attribute\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build classifier excluding k most correlated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "k_most_correlated = corr.iloc[0:k].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_most_correlated)):\n",
    "    print(f\"===========iteration{i+1}===========\")\n",
    "    training_features = list(set(non_sensitive_features).difference(set(k_most_correlated[0:i+1])))\n",
    "    print(\"We exclude from training: \", k_most_correlated[0:i+1])\n",
    "    \n",
    "    # 1. train\n",
    "    post_processing_model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                                    max_depth=10)\n",
    "\n",
    "    post_processing_model.fit(X = train_data[training_features],\n",
    "                              y = train_data[target_column])\n",
    "    \n",
    "\n",
    "    # 2. compute matrix\n",
    "    post_processing_metrics = compute_metric(y_pred= post_processing_model.predict(test_data[training_features]),\n",
    "                                             y_true= test_data[target_column],\n",
    "                                             sensitive_feature = test_data[\"sex\"])\n",
    "    \n",
    "    # 3. append result\n",
    "    post_processing_results = pd.DataFrame(post_processing_metrics,\n",
    "                                           index = [f\"post_prossesing_{i+1}\"]).T\n",
    "    \n",
    "    all_results = pd.concat([all_results, post_processing_results],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-society",
   "language": "python",
   "name": "ml-society"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}