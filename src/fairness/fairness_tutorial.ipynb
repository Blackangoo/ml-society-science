{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Fairness\n",
    "\n",
    "Machine learning (ML) usage is increasing every day. Companies and Organisation build ML model with the perpuse of minimize the humman efford or improve accuracy in varius task. So ML is arguably affects our lives. There is also cases that ML models are used directly on ours personal data.\n",
    "\n",
    "Examples:\n",
    "1. Recomendation system\n",
    "2. Candidate selection e.x college admitions, CV screening\n",
    "3. Loan admitions\n",
    "4. Estimate the propability if someone commits a crime.\n",
    "\n",
    "There are a lot of examples that research demostrate inadvertently discriminating against several polulation groups. The most know is the reaserch Public Republica in Compas Dataset.\n",
    "\n",
    "So the last decade Fairness is becomes one of the most active reachard areas in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Causes of Unfair Models\n",
    "\n",
    "ML bias comes directly from our word dataset.\n",
    "Essentially, this bias comes from human bias existing in training dataset due to historical reasons.\n",
    "\n",
    "Source of bias:\n",
    "1. One example is police record. The record of crimes only come from those crimes observed by police. The police department tends to dispatch more officers to the place that was found to have higher crime rate initially and is thus more likely to record crimes in such regions. Even if people in other regions have higher crime rate later, it is possible that due to less police attention, the police department still record that these regions have lower crime rate. The prediction system trained using data collected in this way tends to have positive bias towards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4. Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5. Analysis of model fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions Equalized Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions Rate Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6. Unawareness\n",
    "Train a Model without the sensitive features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
