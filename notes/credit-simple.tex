\section{Project: Credit risk for mortgages}

Consider a bank that must design a decision rule for giving loans to
individuals. In this particular case, some of each individual's
characteristics are partially known to the bank.  We can assume that
the insurer has a linear utility for money and wishes to maximise
expected utility. Assume that the $t$-th individual is associated with
relevant information $x_t$, sensitive information $z_t$ and a
potential outcome $y_t$, which is whether or not they will default on
their mortgage. For each individual $t$, the decision rule chooses
$a \in \CA$ with probability $\pol(a_t = a \mid x_t)$.

As an example, take a look at the historical data in
\texttt{data/credit/german.data-mumeric}, described in
\texttt{data/credit/german.doc}. Here there are some attributes
related to financial situation, as well as some attributes related to
personal information such as gender and marital status.

\subsection{Baseline analysis.}
\begin{enumerate}
\item To create a decision rule, we first need to build a model. How
  likely is it that individual $t$ repays the loan ($y_t = 1$), given
  their characteristics $x_t$, i.e. what is
  \[
    \Pr(y_t | x_t, z_t).
  \]
\item We can build this model using standard machine learning methods,
  such as a classification algorithm.
\item We will then analyse how different decision rules perform given
  this data. Is it best to simply give a loan to people with a
  probability of returning the loan that is larger than 1/2? Should we just look at the expected return of the loan? If so, then we must take into account the loan rate.
\end{enumerate}

\subsection{Privacy}
(a) Does the existence of this database raise any privacy concerns?

(b) If the database was secret (and only known by the bank), but the credit
decisions were public, how would that affect privacy?

(c) Explain how you would protect the data of the people in the
training set. 

(d) Explain how would protect the data of the people that apply
for new loans.

(e) \emph{Implement} a private mechanism for (c) or (d). In addition,
show how increased privacy affects loan or prediction performance.

\subsection{fairness}

Choose one concept of fairness, e.g. balance of decisions with respect to gender. How can you measure whether your policy is fair? How does the original training data affect the fairness of your policy? To help you in this part of the project, here is a list of guiding questions.

\begin{itemize}
\item Identify sensitive variables. Do the original features already
  imply some bias in data collection?
\item Analyse the data or your decision function with simple
  statistics such as histograms.
\item For group fairness concepts, you can measure the total
  variation of the action (or outcome) distribution for different
  outcomes (or actions) when the sensitive variable varies.
\item Advanced: Using stochastic gradient descent, find a policy that
  balances out fairness and utility.
\end{itemize}

Submit a final report about your project, either as a standalone PDF
or as a jupyter notebook. For this, you can imagine playing the role
of an analyst who submits a possible decision rule to the bank, or
the authorities. You'd like to show that your decision rule is quite
likely to make a profit, that it satisfies \emph{some} standard of
privacy and that it does not unduly discriminate between
applicants. You should definitely point out any possible
\emph{deficiencies} in your analysis due to your assumptions,
methodology, or the available data from which you are drawing
conclusions.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: notes
%%% End:
